# TFLM 라이브러리

TFLM C++ 라이브러리는 TensorFlow Lite와 호환되도록 설계되어 기본적으로는 TFLite에서 정의된 구조를 통해 접근하되 마이크로컨트롤러 한정 라이브러리를 사용하는 시점에 참조됩니다.
마이크로컨트롤러용 헤더파일들의 종류 및 역할은 TensorFlow 공식 문서를 참조한 바에 따라 아래와 같습니다.

![스크린샷, 2021-12-15 19-36-41](https://user-images.githubusercontent.com/76988777/146171164-e422cf9e-e0c4-4762-abe8-ada151421e8d.png)


# FlatBuffer

![스크린샷, 2021-12-15 14-04-48](https://user-images.githubusercontent.com/76988777/146126800-05246d8c-6037-4630-9eda-c1774d478cdc.png)

마이크로컨트롤러 보드에서 모델 추론을 수행하기 위함은 TFLM 라이브러리를 사용하는 목적 중 하나이다. 마이크로컨트롤러 보드는 한정된 용량의 SRAM 및 FLASH 메모리를 가지기 때문에 메모리 사용에 있어 주의를 기울여야 한다. 뿐만 아니라 모델의 구조가 복잡한, 학습된 DNN 모델을 수행하기 위해서는 방대한 양의 연산을 처리해야 하며 학습이 완료된 후 결과물로써 저장된 파일의 용량 역시 상대적으로 크기 때문에 마이크로컨트롤러 기반 추론 환경에 있어 그대로 사용하는 데엔 무리가 있다. 

그렇기 때문에 마이크로컨트롤러 환경에서 실행될 수 있도록 학습된 모델 파일을 적절한 형식인 FlatBuffer로 변환해주어 모델 크기를 줄이고 Tensorflow Lite가 제한적인 연산만을 사용하도록 수정해야 한다(제한된 수행가능 연산 리스트는 all_ops_resolver.cc 파일을 통해 확인할 수 있다.). 

FlatBuffer는 구글에서 개발된 크로스 플랫폼 직렬화 라이브러리로, FlatBuffer를 사용함으로써 메모리 효율이 증가하고 속도가 증가하며, 패킹/언 패킹 없이 직렬화 된 데이터에 엑세스를 할 수 있다. 데이터가 직렬화 되었다는 것은 객체의 내용을 바이트 단위로 변환하여 입출력에 구애받지 않고 파일 또는 네트워크를 통해서 송수신이 가능토록 하는 것을 의미한다. 또한 생성된 코드가 작고 단일 header 파일로 쉽게 통합을 할 수 있다. FlatBuffer를 생성하기 위해 구조를 담은 스키마를 작성해야 하며, 해당 내용이 TFlite에서 작성된 파일은 schema.fbs파일을 통해 참조할 수 있다.

메모리 제한에 관련된 예시를 들자면, STM32-F407IGTX 보드의 경우 128KB 용량의 SRAM을 가지며 1MB용량의 FLASH를 가진다. 그러나 Tensorflow 공식 문서에 따르면, 부동소수점 모델 버전으로 학습된 Mobilenet_V2_1.0_224 의 크기는 14.0Mb이며, TFLite FlatBuffer 및 Tensorflow 고정 그래프가 모두 포함되어 양자화된 버전으로 변환된 Mobilenet_V2_1.0_224의 크기는 3.4Mb이다. 

위 상황의 경우 두 버전의 파일 모두 앞서 언급한 보드의 1MB용량 Flash 메모리에 로드하기엔 터무니없이 큰 용량이므로, 타겟 보드의 제약에 맞는 적절한 모델을 찾는 것 역시 필요하지만, 이 이전에 학습된 모델 파일의 FlatBuffer로의 변환은 마이크로컨트롤러 환경에서의 추론을 준비함에 있어 필수적 요소이다. 

이 문서에서는 학습된 모델을 tflite 파일로 변환하는 과정, 그리고 변환 전 양자화 수준을 나누어 선택하도록 하는 과정은 Tensorflow 공식 문서에 참조되어 있으므로 생략하고, FlatBuffer로의 파일 변환의 필요성을 언급하고자 하였다.

# Setup
TFLM 라이브러리를 사용하여 마이크로컨트롤러를 통해 학습한 모델을 추론하고자 한다. 이때 추론을 하기 전 TFLM에서 정의된, 정형화된 절차를 통해 추후 모델 추론 과정을 성공적으로 수행하기 위한 준비 단계를 거치게 된다. 해당 과정을 묶어 setup 과정이라고 부르자. TFLM에서 setup 과정은 다음 단계들을 수행한다.

![스크린샷, 2021-12-15 14-04-48](https://user-images.githubusercontent.com/76988777/146126800-05246d8c-6037-4630-9eda-c1774d478cdc.png)

1. Flatbuffer model 불러오기



2. interpreter 인스턴스 생성

3. interpreter에 tensor들을 할당


참고자료 : https://www.tensorflow.org/lite/
